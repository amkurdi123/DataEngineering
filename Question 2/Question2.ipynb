{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b13db88-eea3-47fc-8080-2f09be653df0",
   "metadata": {},
   "source": [
    "# Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d965f469-477c-4dbf-8747-8343a18f6ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.operators.email_operator import EmailOperator\n",
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import datetime\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.operators.email_operator import EmailOperator\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv    \n",
    "import psycopg2\n",
    "from psycopg2 import Error\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd686afc-8b2a-41d0-835b-e3798a736fd0",
   "metadata": {},
   "source": [
    "# Chekcing is Faker and MongoClient is installed in the airflow worker and schedular containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b4326b-6e9d-4127-9373-6e453478d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from faker import Faker\n",
    "except:\n",
    "    subprocess.check_call('pip'+ 'install' +'faker')\n",
    "    from faker import Faker\n",
    "\n",
    "try:\n",
    "    from pymongo import MongoClient\n",
    "except:\n",
    "    subprocess.check_call('pip'+ 'install' +'pymongo')    \n",
    "    from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22419cd8-4e15-4685-b89d-165e569c6169",
   "metadata": {},
   "source": [
    "# Building a function to generate dummy data using faker , the function will generate one csv file with 1000 record and place is in /opt/airflow/logs/\n",
    "# This function will be called by a PythonOperator in the DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90efec91-0afc-4f58-903e-7497a1a36681",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################Generating Dummy Data###########################################################################################################\n",
    "def generate_dummy_data():    \n",
    "    output=open('/opt/airflow/logs/data.CSV','w')\n",
    "    fake=Faker()\n",
    "    #Faker.seed(2020)\n",
    "    header=['name','age','country','city','zipcode','state']\n",
    "    mywriter=csv.writer(output)\n",
    "    mywriter.writerow(header)\n",
    "    for r in range(1000):\n",
    "        mywriter.writerow([fake.name(),fake.random_int(min=18,max=80, step=1), fake.country(), fake.city(),fake.zipcode(),fake.state()])\n",
    "    output.close()\n",
    "    print(\"generate dummy data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939ebf77-c005-4a1c-bcc5-4b0d94ce2e08",
   "metadata": {},
   "source": [
    "# Building a function to connect to the Postgres database and create a table called data_set with name,age , country , city , zipcode , and state \n",
    "# it will also read the generated CSV file from the previous step and dump it into the data_set table.\n",
    "# The Postgres Database changes the IP Randomly when the container is up , it usually reserve one of the below IPs , but sometimes it change it to 172.18.0.5 ,if the flow failed trying to access the postgres on once of those IPs , then you will need to change it in this script to point to the right IP.\n",
    "# This function will be called by a PythonOperator in the DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741bc8b4-7604-46a7-a14b-d05de9dd53ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### Creating the table and loading the data into Postgres################################################################################\n",
    "\n",
    "def load_into_postgres():\n",
    "\n",
    "\n",
    "    try:\n",
    "        connection = psycopg2.connect(user = \"airflow\",\n",
    "                                  password = \"airflow\",\n",
    "                                  host = \"172.18.0.4\",\n",
    "                                  port = \"5432\",\n",
    "                                  database = \"postgres\")\n",
    "    except:\n",
    "        connection = psycopg2.connect(user = \"airflow\",\n",
    "                                  password = \"airflow\",\n",
    "                                  host = \"172.18.0.6\",\n",
    "                                  port = \"5432\",\n",
    "                                  database = \"postgres\")\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "\n",
    "    create_table_query = '''CREATE TABLE IF NOT EXISTS data_set\n",
    "    (name  TEXT NOT NULL ,\n",
    "    age  TEXT NOT NULL ,\n",
    "    country  TEXT NOT NULL ,\n",
    "    city TEXT NOT NULL ,\n",
    "    zipcode TEXT NOT NULL ,\n",
    "    state TEXT NOT NULL );'''\n",
    "\n",
    "    cursor.execute(create_table_query)\n",
    "    connection.commit()\n",
    "    f = open(r'/opt/airflow/logs/data.CSV', 'r')\n",
    "    print(f)\n",
    "    print(cursor.copy_from(f, 'data_set', sep=','))\n",
    "    connection.commit()\n",
    "    f.close()\n",
    "    connection.close()\n",
    "    print(\"load the csv file into postgres database\")\n",
    "\n",
    "###################################### Extracting Data - Canada #########################################################################################################\n",
    "\n",
    "def extract_from_postgres_canada():\n",
    "    try:\n",
    "        connection = psycopg2.connect(user = \"airflow\",\n",
    "                                  password = \"airflow\",\n",
    "                                  host = \"172.18.0.4\",\n",
    "                                  port = \"5432\",\n",
    "                                  database = \"postgres\")\n",
    "    except:\n",
    "        connection = psycopg2.connect(user = \"airflow\",\n",
    "                                  password = \"airflow\",\n",
    "                                  host = \"172.18.0.6\",\n",
    "                                  port = \"5432\",\n",
    "                                  database = \"postgres\")\n",
    "    cursor = connection.cursor()\n",
    "    sql = \"COPY (SELECT * FROM data_set WHERE country = 'Canada') TO STDOUT WITH CSV HEADER \"\n",
    "    try:\n",
    "        with open(\"/opt/airflow/logs/canada.csv\", \"w\") as file:\n",
    "          cursor.copy_expert(sql, file)\n",
    "    except psycopg2.Error as e:\n",
    "        print(e)\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    #print(\"extract the data and save it as csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc6f2c2-76e7-4a85-92e5-43c479fa196f",
   "metadata": {},
   "source": [
    "# This function will be extracting data from the postgres database related to country= Brazil and store it in brazil.csv file .\n",
    "# This function will be called by a PythonOperator in the DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a090146-ad5b-4ec6-a193-3b069ed969d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_postgres_brazil():\n",
    "    try:\n",
    "        connection = psycopg2.connect(user = \"airflow\",\n",
    "                                  password = \"airflow\",\n",
    "                                  host = \"172.18.0.4\",\n",
    "                                  port = \"5432\",\n",
    "                                  database = \"postgres\")\n",
    "    except:\n",
    "        connection = psycopg2.connect(user = \"airflow\",\n",
    "                                  password = \"airflow\",\n",
    "                                  host = \"172.18.0.6\",\n",
    "                                  port = \"5432\",\n",
    "                                  database = \"postgres\")\n",
    "    cursor = connection.cursor()\n",
    "    sql = \"COPY (SELECT * FROM data_set WHERE country = 'Brazil') TO STDOUT WITH CSV HEADER \"\n",
    "    try:\n",
    "        with open(\"/opt/airflow/logs/brazil.csv\", \"w\") as file:\n",
    "          cursor.copy_expert(sql, file)\n",
    "    except psycopg2.Error as e:\n",
    "        print(e)\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    #print(\"extract the data and save it as csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d8dca-f630-4943-9f5b-a7fb5f705c90",
   "metadata": {},
   "source": [
    "# This function will be extracting data from the postgres database related to country= canada and store it in canada.csv file .\n",
    "# This function will be called by a PythonOperator in the DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b38f90-9b38-4b42-8a2c-109abbc6a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_postgres_canada():\n",
    "    try:\n",
    "        connection = psycopg2.connect(user = \"airflow\",\n",
    "                                  password = \"airflow\",\n",
    "                                  host = \"172.18.0.4\",\n",
    "                                  port = \"5432\",\n",
    "                                  database = \"postgres\")\n",
    "    except:\n",
    "        connection = psycopg2.connect(user = \"airflow\",\n",
    "                                  password = \"airflow\",\n",
    "                                  host = \"172.18.0.6\",\n",
    "                                  port = \"5432\",\n",
    "                                  database = \"postgres\")\n",
    "    cursor = connection.cursor()\n",
    "    sql = \"COPY (SELECT * FROM data_set WHERE country = 'Canada') TO STDOUT WITH CSV HEADER \"\n",
    "    try:\n",
    "        with open(\"/opt/airflow/logs/canada.csv\", \"w\") as file:\n",
    "          cursor.copy_expert(sql, file)\n",
    "    except psycopg2.Error as e:\n",
    "        print(e)\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecba510-41d3-495d-a87d-f75e18391c1e",
   "metadata": {},
   "source": [
    "# This function will be extracting data from the postgres database related to country= ghana and store it in ghana.csv file .\n",
    "# This function will be called by a PythonOperator in the DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca8f4fb-3b76-42d8-b507-561b8266d093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_postgres_ghana():\n",
    "    try:\n",
    "        connection = psycopg2.connect(user = \"airflow\",\n",
    "                                  password = \"airflow\",\n",
    "                                  host = \"172.18.0.4\",\n",
    "                                  port = \"5432\",\n",
    "                                  database = \"postgres\")\n",
    "    except:\n",
    "        connection = psycopg2.connect(user = \"airflow\",\n",
    "                                  password = \"airflow\",\n",
    "                                  host = \"172.18.0.6\",\n",
    "                                  port = \"5432\",\n",
    "                                  database = \"postgres\")\n",
    "    cursor = connection.cursor()\n",
    "    sql = \"COPY (SELECT * FROM data_set WHERE country = 'Ghana') TO STDOUT WITH CSV HEADER \"\n",
    "    try:\n",
    "        with open(\"/opt/airflow/logs/ghana.csv\", \"w\") as file:\n",
    "          cursor.copy_expert(sql, file)\n",
    "    except psycopg2.Error as e:\n",
    "        print(e)\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    #print(\"extract the data and save it as csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5ba264-b569-484b-8663-dd91583776b2",
   "metadata": {},
   "source": [
    "# This function will transform all the generated\\extracted CSV files into Json files\n",
    "# This function will be called by a PythonOperator in the DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadcf224-314c-4402-85a5-5582a17d105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_json():\n",
    "    df = pd.read_csv(\"/opt/airflow/logs/canada.csv\")\n",
    "    df.to_json(\"/opt/airflow/logs/canada.json\")\n",
    "    print(\"transform the csv data to json for canada\")\n",
    "    df = pd.read_csv(\"/opt/airflow/logs/brazil.csv\")\n",
    "    df.to_json(\"/opt/airflow/logs/brazil.json\")\n",
    "    print(\"transform the csv data to json for brazil\")\n",
    "    df = pd.read_csv(\"/opt/airflow/logs/ghana.csv\")\n",
    "    df.to_json(\"/opt/airflow/logs/ghana.json\")\n",
    "    print(\"transform the csv data to json for ghana\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d036ba-c544-4d20-b3d8-3b7899a01a4f",
   "metadata": {},
   "source": [
    "# This function will prepare the MongoDB and check if the database called Simple_Pipeline exisit or not\n",
    "# This function will be called by a PythonOperator in the DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835c81ae-b149-4f5f-9f58-98eb1ad370dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mongodb_db():\n",
    "    client = MongoClient('mongo:27017',username='root', password='example')\n",
    "    dblist = client.list_database_names()\n",
    "    if \"simple_pipline\" in dblist:\n",
    "        print(\"simple_pipline database exists\")\n",
    "        db = client['simple_pipeline']\n",
    "    else:\n",
    "        print(\"simple_pipline database is created now\")\n",
    "        db = client['simple_pipeline']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c7c678-3443-46a1-a1a4-fab6cb7c05e0",
   "metadata": {},
   "source": [
    "# This function will dump the canada.json document in the canada collection , as part of the Simple_Pipeline database on the MongoDB\n",
    "# This function will be called by a PythonOperator in the DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5b4104-8028-4911-80f7-0161b65b125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_into_mongo_canada():\n",
    "    client = MongoClient('mongo:27017',username='root', password='example')\n",
    "    db = client['simple_pipeline']\n",
    "    collection_canada = db['Canada']\n",
    "    \n",
    "    with open('/opt/airflow/logs/canada.json') as f:\n",
    "        file_data = json.load(f)\n",
    " \n",
    "    collection_canada.insert_one(file_data)\n",
    "    client.close()\n",
    "    print(\"load the canada json file into the mongoDB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faac8bc-8d68-4652-9646-b564adc2a554",
   "metadata": {},
   "source": [
    "# This function will dump the brazil.json document in the brazil collection , as part of the Simple_Pipeline database on the MongoDB\n",
    "# This function will be called by a PythonOperator in the DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d7f6d2-c5ac-4862-8778-da192d456b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_into_mongo_brazil():\n",
    "    client = MongoClient('mongo:27017',username='root', password='example')\n",
    "    db = client['simple_pipeline']\n",
    "    collection_canada = db['Brazil']\n",
    "    \n",
    "    with open('/opt/airflow/logs/brazil.json') as f:\n",
    "        file_data = json.load(f)\n",
    " \n",
    "    collection_canada.insert_one(file_data)\n",
    "    client.close()\n",
    "    print(\"load the brazil json file into the mongoDB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707cfbce-f178-42db-a8da-3439e3a4cd4e",
   "metadata": {},
   "source": [
    "# This function will dump the ghana.json document in the ghana collection , as part of the Simple_Pipeline database on the MongoDB\n",
    "# This function will be called by a PythonOperator in the DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fa5b41-8abf-4c11-9a1c-de5fbf65f516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_into_mongo_ghana():\n",
    "    client = MongoClient('mongo:27017',username='root', password='example')\n",
    "    db = client['simple_pipeline']\n",
    "    collection_canada = db['Ghana']\n",
    "    \n",
    "    with open('/opt/airflow/logs/ghana.json') as f:\n",
    "        file_data = json.load(f)\n",
    " \n",
    "    collection_canada.insert_one(file_data)\n",
    "    client.close()\n",
    "    print(\"load the ghana json file into the mongoDB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075ec588-2f5c-453d-9bcf-ba3b0860324a",
   "metadata": {},
   "source": [
    "# Building the DAG Object and call is Simple_Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f144b9e-1591-452f-9c07-16bff0d3656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipline_dag = DAG(\n",
    "    dag_id = 'Simple_Pipline',\n",
    "    #schedule_interval = \"0 0 * * *\",\n",
    "    start_date=datetime(2021,4,5)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879d03a4-9923-4824-b0de-6b42ead15e83",
   "metadata": {},
   "source": [
    "# Building the PythonOperators - Each one will call the designated callable function from the above defined ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb271fe-03cd-44a8-be99-0101c5c93cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preparation_1 = PythonOperator(task_id='prepare_dummy_data',\n",
    "                             python_callable=generate_dummy_data,\n",
    "                             dag=pipline_dag)\n",
    "                             \n",
    "data_preparation_2 = PythonOperator(task_id='dump_into_postgres',\n",
    "                             python_callable=load_into_postgres,\n",
    "                             dag=pipline_dag)\n",
    "\n",
    "data_extraction_canada = PythonOperator(task_id='extract_from_postgres_canada',\n",
    "                             python_callable=extract_from_postgres_canada,\n",
    "                             dag=pipline_dag)\n",
    "data_extraction_brazil = PythonOperator(task_id='extract_from_postgres_brazil',\n",
    "                             python_callable=extract_from_postgres_brazil,\n",
    "                             dag=pipline_dag)\n",
    "data_extraction_ghana = PythonOperator(task_id='extract_from_postgres_ghana',\n",
    "                             python_callable=extract_from_postgres_ghana,\n",
    "                             dag=pipline_dag)\n",
    "data_transformation = PythonOperator(task_id='transform_data_structure',\n",
    "                             python_callable=csv_to_json,\n",
    "                             dag=pipline_dag)\n",
    "prepare_warehouse = PythonOperator(task_id='prepare_warehouse',\n",
    "                             python_callable=prepare_mongodb_db,\n",
    "                             dag=pipline_dag)\n",
    "data_load_canada = PythonOperator(task_id='load_data_canada',\n",
    "                             python_callable=load_json_into_mongo_canada,\n",
    "                             dag=pipline_dag)\n",
    "data_load_brazil = PythonOperator(task_id='load_data_brazil',\n",
    "                             python_callable=load_json_into_mongo_brazil,\n",
    "                             dag=pipline_dag)\n",
    "data_load_ghana = PythonOperator(task_id='load_data_ghana',\n",
    "                             python_callable=load_json_into_mongo_ghana,\n",
    "                             dag=pipline_dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffce4af-8970-4f50-937a-baaa2e089013",
   "metadata": {},
   "source": [
    "## Finally , designing the flow of the pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71af90c-9f99-4cd1-97d6-4d1709c2bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preparation_1 >> data_preparation_2 >> data_extraction_canada >> data_transformation >> prepare_warehouse\n",
    "data_preparation_2 >> data_extraction_brazil >> data_transformation >> prepare_warehouse\n",
    "data_preparation_2 >> data_extraction_ghana >> data_transformation >> prepare_warehouse\n",
    "prepare_warehouse >> data_load_canada\n",
    "prepare_warehouse >> data_load_brazil\n",
    "prepare_warehouse >> data_load_ghana"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
